{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1458b936-1358-4d88-9082-0620e450411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community openai tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6a72d8-c7ab-4393-ad0d-9edc06159be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimon import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835191a-cb60-405f-8e89-57a7bfe02b9e",
   "metadata": {},
   "source": [
    "# Create an AIMon client\n",
    "\n",
    "This creates the AIMon client that will be used for the various different operations under evaluation and continuous monitoring of production applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790a85fa-3026-4ea9-94e4-8bbb22cdb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AIMon client. You would need an API Key (that can be retrieved from the UI in your user profile). \n",
    "import os\n",
    "am_api_key = os.getenv(\"AIMON_API_KEY\")\n",
    "aimon_client = Client(auth_header=\"Bearer {}\".format(am_api_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b7c9a-0001-4ada-8699-9350d27b735d",
   "metadata": {},
   "source": [
    "# Generative AI Model\n",
    "\n",
    "A model is a generative model that will be powering your application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90afb156-7538-4166-9c3b-0e5a131087dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT4', 'GPT-4', 'GPT-4o', 'Llama-2', 'Llama3', 'model_type', 'string']\n"
     ]
    }
   ],
   "source": [
    "# Pick from existing model model types in the company. These are created by you or other member of your organization.\n",
    "# The AIMon client has a convenience function to easily retrieve this.\n",
    "list_model_types = aimon_client.models.list()\n",
    "print(list_model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808db089-f86e-4f79-9dc5-67e1a5fa35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the AIMon client, create or get a model for a given model type. \n",
    "# This API will automatically create a new model if it does not exist.\n",
    "my_model = aimon_client.models.create(\n",
    "    name=\"my_gpt4_model_fine_tuned\", \n",
    "    type=\"GPT-4\", \n",
    "    description=\"This model is a GPT4 based model and is fine tuned on the awesome_finetuning dataset\", \n",
    "    metadata={\"model_s3_location\":\"s3://bucket/key\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce5d51-4ceb-4a50-9561-681a1059076e",
   "metadata": {},
   "source": [
    "# LLM application\n",
    "\n",
    "This is to create or get an application that is using the above model. Each application is versioned i.e., each application is associated with a particular model for a given version of the application. When you use a different model for the same application, AIMon will automatically increment the version of the application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22794a7c-8193-4c10-b633-fe4d0d4277d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the AIMon client, create or get an existing application\n",
    "new_app = aimon_client.applications.create(\n",
    "    name=\"my_application_sept_4_2024\", \n",
    "    model_name=my_model.name, \n",
    "    stage=\"evaluation\", \n",
    "    type=\"summarization\", \n",
    "    metadata={\"application_url\": \"https://acme.com/summarization\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a4ade-8083-4f67-a762-430f205c83b5",
   "metadata": {},
   "source": [
    "### Core LLM Application code\n",
    "\n",
    "The below example uses Langchain to do summarization of documents using OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11e5fc3a-da44-4fda-b29a-a691569487d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af98abfc-c04d-4cc7-ba3c-62f550de0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lanchain app example\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "def run_application(new_app, source_text, prompt=None, user_query=None, eval_run=None):\n",
    "    # Split the source text\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    texts = text_splitter.split_text(source_text)\n",
    "    \n",
    "    # Create Document objects for the texts\n",
    "    docs = [Document(page_content=t) for t in texts[:3]]\n",
    "    \n",
    "    # Initialize the OpenAI module, load and run the summarize chain\n",
    "    llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "    summary = chain.run(docs)\n",
    "\n",
    "    payload = {\n",
    "                \"application_id\": new_app.id,\n",
    "                \"version\": new_app.version,\n",
    "                \"context_docs\": [d.page_content for d in docs],\n",
    "                \"output\": summary,\n",
    "                \"actual_request_timestamp\": \"09/04/2024, 13:32:16\"\n",
    "            }\n",
    "    if prompt:\n",
    "        payload[\"prompt\"] = prompt\n",
    "    if user_query:\n",
    "        payload[\"user_query\"] = user_query\n",
    "    if eval_run is None and new_app.stage == 'evaluation':\n",
    "        raise Exception(\"Evaluation and run ID missing for an application that is in evaluation mode.\")\n",
    "    if eval_run is not None:\n",
    "         payload[\"evaluation_id\"] = eval_run.evaluation_id\n",
    "         payload[\"evaluation_run_id\"] = eval_run.id\n",
    "    # print(payload)\n",
    "    # Analyze quality of the generated output using AIMon\n",
    "    res = aimon_client.analyze.create(\n",
    "        body=[payload]\n",
    "    )\n",
    "    print(\"Aimon response: {}\\n\".format(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b942b58-1d28-4efc-9aef-3e360740a6ca",
   "metadata": {},
   "source": [
    "# Evaluation of the LLM Application\n",
    "\n",
    "Before deploying the application to production, it is a good idea to test it end to end with either a curated golden dataset or a snapshot of production traffic. In this section, we will demonstrate how AIMon can assist you to perform these tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e362f3-7040-4648-8a07-991971c0705d",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "The dataset should be a CSV file with these columns: \n",
    " * \"prompt\": This is the prompt used for the LLM\n",
    " * \"user_query\": This the query specified by the user \n",
    "* \"context_docs\": These are context documents that are either retrieved from a RAG or through other methods. \n",
    "                  For tasks like summarization, these documents could be directly specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34881793-3778-4114-8066-7c41cfb6d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Create a new datasets\n",
    "file_path1 = \"./test_evaluation_dataset_aug_2024_1.csv\"\n",
    "file_path2 = \"./test_evaluation_dataset_aug_2024_2.csv\"\n",
    "\n",
    "dataset_data_1 = json.dumps({\n",
    "    \"name\": \"test_evaluation_dataset_1.csv\",\n",
    "    \"description\": \"This is one custom dataset\"\n",
    "})\n",
    "\n",
    "dataset_data_2 = json.dumps({\n",
    "    \"name\": \"test_evaluation_dataset_2.csv\",\n",
    "    \"description\": \"This is another custom dataset\"\n",
    "})\n",
    "\n",
    "with open(file_path1, 'rb') as file1:\n",
    "    dataset1 = aimon_client.datasets.create(\n",
    "        file=file1,\n",
    "        json_data=dataset_data_1\n",
    "    )\n",
    "\n",
    "with open(file_path2, 'rb') as file2:\n",
    "    dataset2 = aimon_client.datasets.create(\n",
    "        file=file2,\n",
    "        json_data=dataset_data_2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7cc2795-6010-479a-90c0-bd275589f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = aimon_client.datasets.list(name=\"test_evaluation_dataset_1.csv\")\n",
    "dataset2 = aimon_client.datasets.list(name=\"test_evaluation_dataset_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57718d53-ff05-40a2-ba35-db3964f868ee",
   "metadata": {},
   "source": [
    "### Dataset Collection\n",
    "\n",
    "You can define a collection of evaluation datasets for ease of use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dbceabd-a6a2-4671-959d-ca15025841b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset collection\n",
    "dataset_collection = aimon_client.datasets.collection.create(\n",
    "    name=\"my_first_dataset_collection_aug_9_2024_5_12\", \n",
    "    dataset_ids=[dataset1.sha, dataset2.sha], \n",
    "    description=\"This is a collection of two datasets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72f64c-37c1-4500-b6c1-05793f03f66a",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "An evaluation is associated with a dataset collection and an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7991bf-a7b5-42f6-9a45-08c66c98d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the AIMon client, create a new evaluation\n",
    "evaluation = aimon_client.evaluations.create(\n",
    "    name=\"offline_evaluation_fine_tuned_model\", \n",
    "    application_id=new_app.id, \n",
    "    model_id=my_model.id, \n",
    "    dataset_collection_id=dataset_collection.id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea8115-192f-4bfd-a763-b1513c5ea7b9",
   "metadata": {},
   "source": [
    "### New Run\n",
    "A \"run\" is an instance of an evaluation that you would like to track metrics against. You could have multiple runs of the same evaluation. This is typically done in a CI/CD context where the same evaluation would run at regular intervals. Since LLMs are probabilitic in nature, they could produce different outputs for the same query and context. It is a good idea to run the evaluations regularly to understand the variations of outputs produced by your LLMs. In addition, runs give you the ability to choose different metrics for each run. \n",
    "\n",
    "Metrics are specified using the `metrics_config` parameter in the format shown below. The keys indicate the type of metric computed and the values are the specific algorithms used to compute those metrics. For most cases, we recommend using the `default` algorithm.\n",
    "\n",
    "Tags allow you to specify metadata like the application commit SHA or other key-value pairs that you want to insert for analytics purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b76a493-8738-40ba-a722-e22425c37e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the AIMon client, create a new evaluation run. \n",
    "eval_run = aimon_client.evaluations.run.create(\n",
    "    evaluation_id=evaluation.id, \n",
    "    metrics_config={'hallucination': 'default', 'toxicity': 'default', 'conciseness': 'default', 'completeness': 'default'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27198336-2338-4c37-8bc0-8435cdd7eaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_run.evaluation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2706102-672d-4e11-b77b-c1110a31772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/300zf44j5v9c43jpqdbhkl0h0000gn/T/ipykernel_28512/2068912279.py:16: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
      "/var/folders/7l/300zf44j5v9c43jpqdbhkl0h0000gn/T/ipykernel_28512/2068912279.py:18: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  summary = chain.run(docs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aimon response: AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200)\n",
      "\n",
      "Aimon response: AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200)\n",
      "\n",
      "Aimon response: AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200)\n",
      "\n",
      "Aimon response: AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200)\n",
      "\n",
      "Aimon response: AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200)\n",
      "\n",
      "Aimon response: AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200)\n",
      "\n",
      "Aimon response: AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200)\n",
      "\n",
      "Aimon response: AnalyzeCreateResponse(message='Data successfully sent to AIMon.', status=200)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all records from the datasets\n",
    "dataset_collection_records = []\n",
    "for dataset_id in dataset_collection.dataset_ids:\n",
    "    dataset_records = aimon_client.datasets.records.list(sha=dataset_id)\n",
    "    dataset_collection_records.extend(dataset_records)\n",
    "\n",
    "# Run the application on all the records in the dataset collection. \n",
    "for record in dataset_collection_records:\n",
    "    # Run the application code\n",
    "    run_application(new_app, record['context_docs'], record['prompt'], record['user_query'], eval_run=eval_run)\n",
    "    \n",
    "# You can view metrics for your application on the UI: https://www.app.aimon.ai/llmapps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3305f3",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "604b5c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Metrics: Evaluation(metric_name=None, timestamp=None, value=None, avg_completeness_score=0.8942857142857142, avg_conciseness_score=0.8360000000000001, avg_context_doc_length=363.14285714285717, avg_hallucination_score=0.04690142857142857, avg_instruction_adherence_score=0.0, avg_toxicity_scores={'identity_hate': 0.11006870120763779, 'insult': 0.30732529716832296, 'obscene': 0.28974676557949613, 'severe_toxic': 0.05291888649974551, 'threat': 0.12063558399677277, 'toxic': 0.11930480173655919})\n"
     ]
    }
   ],
   "source": [
    "# Get metrics by application\n",
    "app_metrics = aimon_client.applications.evaluations.metrics.retrieve(application_name=new_app.name)\n",
    "\n",
    "# Get metrics by evaluation\n",
    "eval_metrics = aimon_client.applications.evaluations.metrics.get_evaluation_metrics(evaluation_id=evaluation.id, application_name=new_app.name)\n",
    "\n",
    "# #Get metrics by evaluation run\n",
    "# eval_run_metrics = aimon_client.applications.evaluations.metrics.get_evaluation_run_metrics(evaluation_id=40, evaluation_run_id=268, application_name=\"my_first_dataset_collection_aug_9_2024_5_12\")\n",
    "# quality_metrics = eval_run_metrics.evaluations[0].quality_metrics\n",
    "\n",
    "print(f\"Quality Metrics: {eval_metrics.evaluations[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8070363-c9e3-497c-a0de-03b68bd9b49c",
   "metadata": {},
   "source": [
    "# Production\n",
    "\n",
    "Once you have built enough confidence through your evaluations of your application, you can deploy it to production. AIMon gives you the ability to continuously monitor your application for the configured metrics in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f380d48-6c7c-4881-9301-dc05c4060e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the AIMon client, create or get an existing application\n",
    "new_app_prod = aimon_client.applications.create(\n",
    "    name=\"my_application_sept_4_2024\", \n",
    "    model_name=my_model.name, \n",
    "    stage=\"production\", \n",
    "    type=\"summarization\", \n",
    "    metadata={\"application_url\": \"https://acme.com/summarization\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb81a8ad-e10a-43f5-9682-844d9ab2ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"\"\"\n",
    "Large Language Models (LLMs) have become integral to automating and enhancing various business processes. \n",
    "However, a significant challenge these models face is the concept of \\\"hallucinations\\\" - outputs that, \n",
    "although fluent and confident, are factually incorrect or nonsensical. For enterprises relying on AI \n",
    "for decision-making, content creation, or customer service, these hallucinations can undermine credibility, \n",
    "spread misinformation, and disrupt operations. Recently, AirCanada lost a court case due to hallucinations \n",
    "in its chatbot [7]. Also, the 2024 Edelman Trust Barometer reported a drop in trust in AI companies from \n",
    "61% to 53% compared to 90% 8 years ago [8]. Recognizing the urgency of the issue, we have developed a \n",
    "state-of-the-art system designed for both offline and online detection of hallucinations, ensuring higher \n",
    "reliability and trustworthiness in LLM outputs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0dc9d7-e0b0-4a05-8967-0e06179486a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_application(new_app_prod, source_text, prompt=\"Langhchain based summarization of documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0b43d-5b23-454b-bac8-a169a678ed22",
   "metadata": {},
   "source": [
    "# Delete Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34330fd5-316c-4809-a1c9-25060c16ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_resp = aimon_client.applications.delete(\n",
    "    name=\"my_application_sept_4_2024\", \n",
    "    version=\"0\", \n",
    "    stage=\"evaluation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ab1bc6a-5d5a-477d-a8cd-3448092030e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Application my_application_sept_4_2024 deleted successfully.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_resp.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dcb8f6-dc08-444b-b6c2-1316e453d836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
